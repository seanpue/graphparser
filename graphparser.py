# the prev and next classes turned into lists. Had to correct for that
#from myparser import Parser # for now, just load yaml rules via Parser
import networkx as nx
from collections import namedtuple
import re
import unicodedata
ParserRule = namedtuple('ParserRule', ['production','prev_classes','prev_tokens','tokens','next_tokens','next_classes',
                            'prev','next','match_tokens'
])

ParserOutput = namedtuple('ParserOutput', ['matches','output']) 


class GraphParser:


    def load_yaml(self,filename):
        """
        Reads any yaml file and returns it as object.
        filename: name of file
        returns yaml
        """
        import yaml
        stream = file(filename)
        return yaml.load(stream)

    debug=False
        
    
    def unescape_unicode_charnames(self,s):
        """
        Takes \N{charname} in production rule and turns to Unicode string.
        """
        def get_unicode_char(matchobj):
            s = matchobj.group(0)
            m = re.match(r'\\N{(.+)}',s)
            char = unicodedata.lookup(m.group(1))
            return char
        return re.sub(r'\\N{.+?}',get_unicode_char,s)

    def compare_rules(x,y):
        """
        Compares parser rules x,y and sees which has more tokens or conditions (prev, next)
        """
    
        diff = 10*(len(y['tokens'])-len(x['tokens']))   
        if diff != 0:
            return diff
        (x_conds, y_conds) = (0,0)
        for cond in ('prev','next'):
            if cond in x: x_conds +=len(x[cond])
            if cond in y: y_conds +=len(y[cond])
        for cond in ('prev_classes', 'next_classes'):
            if cond in x: x_conds += len(x[cond])
            if cond in y: y_conds += len(y[cond])
        return y_conds - x_conds # see if one has  more <classes>

    def onmatch_rules_from_yaml_data(self,rules_raw):
        ''' Some quick code to generate the onmatch rules. It only relies on classes '''
        ''' returns a tuple ( (left_classes,right_classes) , prod)
            rules are classes '''
        onmatch_rules = [] # clean list of rule
    #    print "rules_r is"+str(rules_raw)
        debug=True

        match_rules = []
        for key in rules_raw:
            assert len(key)==1
            rule = key.keys()[0]
            prod_orig = key[rule]
            prod = self.unescape_unicode_charnames(prod_orig)
            m= re.match('([^+]+)\+([^+]+)$',rule)
            assert m
            l = m.group(1) #left
            r =m.group(2)  #right
            cl_l=re.findall('(?<=<)[^<]+(?=>)',l)
            cl_r=re.findall('(?<=<)[^<]+(?=>)',r)
            onmatch_rules.append(( (cl_l, cl_r) , prod ))
        return(onmatch_rules)
    
    def rules_from_yaml_data(self,rules_raw):
        """
        Returns sorted and usable parser rules from yaml file. 

        rules_raw is a dictionary (loaded from yaml):
            key: <previous token class> token token <next token class> 
            value: production

        previous class and next class of token are optional. 
        The production allows \N{unicode char name} strings.

        Output of return is list of rules containing:
            prev_class: previous token class [optional, will look behind prev]
            prev:       previous tokens [optional]
            tokens:     list of tokens
            next:       next token class [optional]
            next_class: next token class [option, will look ahead of next]
            production: string production

        """
        rules = [] # clean list of rule
        for key in rules_raw:
            rule = {}           #1       #2        #3
            _  ='(?:'   
            _ +='\('
            _ +='((?:\s?<.+?>\s+)+)?'# '(?:\s?<(.+?)>\s+)?' # group 1, prev class (in brackets indicating cluster)
            _ +='(.+?)\s?' # group 2, prev tokens (to be split)
            _ +='\) '
            _ +='|' # either a cluster or a particular previous class (Could add additional support, e.g. class or paretic.
            _ +='((?:\s?<.+?>\s+)+)?' # group 3, prev class (not in cluster)
            _ +=')?'
            _ += '(.+?)' # group 4, tokens
            _ += '(?:' # cluster for following tokens, clusters 
            _ += ' \('
            _ += '\s?(.+?)' # group 5, next tokens
            _ += '((?:\s?<.+?>\s+?)+)?' # group 6, next class
            _ += '\s?\)'
            _ += '|'
            _ += ' ((?:<.+?>\s?)+)?' # group 7, follo
            _ += ')?$'
        
            m = re.match (_, key, re.S)
            assert (m is not None)
            if m.group(1): rule['prev_classes'] = re.findall('<(.+?)>',m.group(1))
            if m.group(2): rule['prev_tokens'] = m.group(2).split(' ')
            if m.group(3): rule['prev_classes'] =  re.findall('<(.+?)>',m.group(3))
            if m.group(4)==' ':
                rule['tokens'] = ' '
            else:
                rule['tokens'] = m.group(4).split(' ')
            if m.group(5): rule['next_tokens'] = m.group(5).split(' ')
            if m.group(6): rule['next_classes'] = re.findall('<(.+?)>',m.group(6))
            if m.group(7): rule['next_classes'] = re.findall('<(.+?)>',m.group(7))
            
            rule['production'] = self.unescape_unicode_charnames(rules_raw[key])
            rules.append(rule)

        return rules
    def generate_token_match_re(self):
        '''
        Create regular expression from Parser.tokens sorted by length

        Adds final "." in case nothing found
        '''

        tokens = self.tokens.keys()
        sorted_tokens = sorted(tokens, key=len, reverse=True)
        escaped_tokens = map(re.escape, sorted_tokens)
        tokens_re_string = '|'.join(escaped_tokens)+'|.' # grab unknowns
        return re.compile(tokens_re_string, re.S)


    def __init__(self, yaml_file='', data=None, blank=' '):

        assert data == None # not implemented yet
        data = self.load_yaml(yaml_file)
        self.dict_rules = self.rules_from_yaml_data(data['rules'])
        self.rules = self.rules_to_tuple(self.dict_rules)
        self.onmatch_rules = self.onmatch_rules_from_yaml_data(data['onmatch'])
#        self.tokens = self.old_parser.tokens
        self.tokens = data['tokens']
        self.token_match_re = self.generate_token_match_re()
        
        DG= self.make_graph()
        self.sorted_out_edges = self.get_sorted_out_edges(DG) # edges are arranged by node and then by weight
#        for i in DGself.
        self.DG = DG
        self.blank = blank # what token is assumed at before and after tokens
    
    def rules_to_tuple(self,rules):
        new_rules = []
        for r in rules:
            left = 0
            for x in ['prev_tokens', 'prev_classes']:
                if x in r:
                    left+=len(r[x])
            right=0
            
            for x in ['tokens', 'next_tokens','next_classes']:
                if x in r:
                    right+=len(r[x])
#            left = len(r.get('prev_tokens'))+len(r.get('prev_classes'))
#            right = len(r['tokens']) +len(r.get(next_tokens))+len(r.get(next_classes))#next_classes)
            match_tokens=[]
            for x in ['prev_tokens','tokens','next_tokens']:
                if r.get(x):
                    match_tokens.extend(r.get(x))
#            match_tokens = prev_tokens+tokens+next_tokens
            pr = ParserRule(production = r['production'], 
                            prev_classes = r.get('prev_classes'),#change here
                            prev_tokens = r.get('prev_tokens'),
                            tokens = r['tokens'],
                            next_tokens = r.get('next_tokens'),
                            next_classes = r.get('next_classes'),
                            prev = left,
                            next = right,
                            match_tokens = match_tokens
                            ) # change here
            new_rules.append(pr)
        return tuple(new_rules)
        
    def tokenize(self,input):
        return self.token_match_re.findall(input)

    def make_graph(self): # turns rules into graph
        DG = nx.DiGraph()
        DG.add_node(0,token=None) #base
        def found_token_at_node(t, n_id):
            for s in DG.successors(n_id):
        #        print s,DG[s]
                if DG.node[s].get('token')==t:
                    return s
            return None

        def weight_of_rule(r): # and here namedtuples get clunky? 
            weight=5 # start at five and subtract.
            if r.prev_classes:
                weight+=-1
            if r.prev_tokens:
                weight+=-1
            if r.next_classes:
                weight+=-1
            if r.next_tokens:
                weight+=-1
            return weight # when we navigate, we will go from low to high in the edges

        token_nodes = []
        match_nodes = []
        for rule in self.rules:
            curr_node = 0
            for t in rule.tokens:
                found = found_token_at_node(t,curr_node)
                if found: 
                    curr_node = found
                    continue
                else:
                    new_node = len(DG.nodes()) 
                    DG.add_node(new_node, token=t)
                    token_nodes.append(new_node) # save in list to be used in drawing for labels, color, etc.
        #            print DG.node[new_node],t
                    DG.add_edge(curr_node,new_node,weight=0) # if not a match node, then no weight, so will try first
                    curr_node = new_node
                
            rule_weight = weight_of_rule(rule) # this will be 1/4 for 4 constraints, 1/3 for 3 constraints, etc.

            new_match_node = len(DG.nodes()) 
            DG.add_node(new_match_node, rule=rule, found=rule.production)
            match_nodes.append(new_match_node)
            DG.add_edge(curr_node, new_match_node, weight=rule_weight,rule=rule) # for now, just copy the rule 
        return DG

    def get_sorted_out_edges(self,g):
        '''
        Returns dictionary from DG containing list of out edges  sorted by weight
        '''
        from collections import defaultdict
        out_edges = defaultdict(list)

        for e in g.edges(data=True):
            start,end,data = e
            out_edges[start].append(e)
        sorted_out_edges = defaultdict(list)
        for k,edges in out_edges.iteritems():
            sorted_edges = sorted(edges, key =lambda x: x[2]['weight'])
            sorted_out_edges[k] = sorted_edges
        return sorted_out_edges
        
    def match_rule(self,rule, tkns, t_i, level):
        parser_tokens = self.tokens
        r_tkns=[]
        if rule.prev_tokens:
            i_start =t_i - len(rule.prev_tokens)
            r_tkns+=rule.prev_tokens # could save this as match offset
        else:
            i_start = t_i
        r_tkns +=rule.tokens
        if rule.next_tokens:
            r_tkns += rule.next_tokens
        if not all(r_tkns[i] == tkns[i_start+i] for i in range(len(r_tkns)) ):
            return False
        if rule.prev_classes:
            prev_classes = rule.prev_classes[::-1] # reverse these
            if i_start - len(prev_classes) < -1: 
                return False
            to_match = ([' ']+tkns)[i_start+1-len(prev_classes):i_start+1][::-1]
            if not all(prev_classes[i] in self.tokens[to_match[i]] for i in range(len(prev_classes))): 
                return False
        if rule.next_classes:
            next_classes = rule.next_classes
            if i_start + len(r_tkns)+len(next_classes) > len(tkns)+1:
                return False
            to_match = tkns[i_start+len(r_tkns):i_start+len(r_tkns)+len(next_classes)] + [' ']
            if not all(next_classes[i] in self.tokens[to_match[i]] for i in range(len(next_classes))): 
                return False
        return True
        

    def match_first_at(self,tokens, token_i):
        def descend_node(curr_node, level):
            for edge in self.sorted_out_edges[curr_node]:#sorted(list(self.DG.edges(curr_node, data=True)), key=lambda x:x[2]['weight'] ):
                next_node = edge[1]
                if edge[2].get('rule'): # if the edge has a rule
                    if self.match_rule(edge[2]['rule'], tokens, token_i,level)==False:
                        continue # skip it
                if self.DG.node[next_node].get('rule'): # matched nodes have found and rule
                    return self.DG.node[next_node].get('rule')
                if token_i+level < len(tokens): # do not descend if at end of road
                    if self.DG.node[next_node].get('token')==tokens[token_i+level]:
                        d =descend_node(next_node, level+1)
                        if d:
                            return d
        return descend_node(0,0)        

    def parse(self,string):
        t_i = 0
        tkns = self.tokenize(string)
        output = ''
        matches = []
        while t_i < len(tkns):
            m = self.match_first_at(tkns, t_i)
            if m==None:
                print "error in string",string,len(string)
            assert m != None # for now, croak on error
            matches.append(m)
            if self.onmatch_rules:
                mtkns = [' ']+tkns+[' ']
                mt_i = t_i+1
                for mr in self.onmatch_rules:
                    (classes,p)=mr
                    (l_c,r_c)=classes
                    if mt_i < len(l_c) or mt_i+len(r_c)>len(mtkns):
                        continue
                    classes_to_test_l = [self.tokens[c] for c in mtkns[mt_i-len(l_c):mt_i]]
                    classes_to_test_r = [self.tokens[c] for c in mtkns[mt_i:mt_i+len(r_c)]]
                    if not all(l_c[i] in classes_to_test_l[i] for i in range(len(l_c))):
                        continue
                    if not all(r_c[i] in classes_to_test_r[i] for i in range(len(r_c))):
                        continue
                    output += p
                    break # break out of for loop
            output+=m.production
            t_i += len(m.tokens)
        return ParserOutput(output=output,matches=matches)
    
    def match_all_at(self,tokens, token_i):
        matches = []
        def descend_node(curr_node, level):
            for edge in sorted(list(self.DG.edges(curr_node, data=True)), key=lambda x:x[2]['weight'] ):
                next_node = edge[1]
                if edge[2].get('rule'): # if the edge has a rule
                    if self.match_rule(edge[2]['rule'], tokens, token_i,level)==False:
                        continue
                if self.DG.node[next_node].get('rule'): # matched nodes have found and rule
                            matches.append(self.DG.node[next_node].get('rule'))
                            continue
                if token_i+level < len(tokens):
                        if self.DG.node[next_node].get('token')==tokens[token_i+level]:
                            descend_node(next_node, level+1)
        descend_node(0,0)        
        return matches

if __name__ == '__main__':
    urdup = GraphParser('settings/devanagari.yaml')
    x=urdup.parse('kih baskih')
    print x.output